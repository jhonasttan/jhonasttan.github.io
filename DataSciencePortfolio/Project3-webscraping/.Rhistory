cbind("Log Odds" = glm.case$coefficients,
"Odds" = exp(glm.case$coefficients))
glm.case$coefficients
confint(glm.case)
confint.default(glm.case)
confint(logit.overall) #For logistic regression objects, the confint() function
confint.default(logit.overall) #To generate confidence intervals for logistic
exp(confint(glm.case))
exp(confint.default(glm.case))
names(case2002)
glm.minus.bird <- glm(LC ~. -BKBird, data=case2001, family = "binomial")
glm.minus.bird <- glm(LC ~. -BKBird, data=case2002, family = "binomial")
glm.minus.bird <- glm(LC ~. -BK, data=case2002, family = "binomial")
plot(glm.minus.bird)
summary(glm.minus.bird)
influencePlot(glm.minus.bird)
glm.minus.bird$df.residual
reduced.deviance = glm.minus.bird$deviance #Comparing the deviance of the reduced
reduced.df = glm.minus.bird$df.residual    #model (the one without rank) to...
full.deviance = glm.case$deviance #...the deviance of the full model (the
full.df = glm.case$df.residual    #one with the rank terms).
pchisq(reduced.deviance - full.deviance,
reduced.df - full.df,
lower.tail = FALSE)
anova(glm.case, glm.minus.bird, test = "Chisq") #anova example
names(case2002)
glm.bk.yr <- glm(LC ~ BK + YR, data = case2002, family = "binomial")
anova(glm.minus.bird, glm.bk.yr, test = 'Chisq')
anova(glm.case, glm.bk.yr, test = 'Chisq')
AIC(glm.case,glm.minus.bird,glm.bk.yr)
names(case2002)
names(GradSchools)
summary(case2002$BK)
class(case2002$BK)
head(case2002$BK)
newdata.LC = with(case2002, data.frame(YR = mean(YR),
BK = factor("NoBird")))
newdata.LC
newdata
newdata = with(GradSchools, data.frame(gre = mean(gre),
gpa = mean(gpa),
rank = factor(1:4)))
newdata
predict(glm.bk.yr, newdata.LC)
predict.newdata.LM <- predict(glm.bk.yr, newdata.LC)
predict.newdata.LM <- predict(glm.bk.yr, newdata.LC, type = 'response')
predict.newdata.LM
summary(case2002$YR)
newdata.LC.2 = with(case2002, data.frame(YR = 0,
BK = factor("NoBird")))
predict.newdata.LM.2 <- predict(glm.bk.yr, newdata.LC.2, type = 'response')
predict.newdata.LM.2
table(truth = case2002$LC, prediction = glm.bk.yr)
table(truth = case2002$LC, prediction = glm.bk.yr$fitted.values)
LC.predicted = round(glm.bk.yr$fitted.values)
table(truth = case2002$LC, prediction = LC)
table(truth = case2002$LC, prediction = LC.predicted)
(85 + 22) / (85 + 22 + 13 + 27)
install.packages("fBasics")
#Load data rendered by Scrapy
library(purrr)
library(ggplot2)
library(plotly)
library(psych)
library(DT)
library(reshape2)
require(FactoMineR)
df_sb <- read.csv('./starbucks/StarbucksStores.txt', stringsAsFactors = FALSE)
df_sb_unique <- df_sb[!duplicated(df_sb), ]
#PCA procedure to determine which amenities should be enabled for filtering on the map
df_sb_pca <- df_sb_unique[,c(11:ncol(df_sb_unique))] #select the amenities columns
df_sb_pca <- df_sb_pca[,-ncol(df_sb_pca)] #drop the last column 'AmeOther'
#head(df_sb_unique[,1:7], 1)
setwd("~/Documents/DataScience/bootcamp7/githubcrush/bootcamp007_project/Project3-WebScraping/JhonasttanRegalado")
#Load data rendered by Scrapy
library(purrr)
library(ggplot2)
library(plotly)
library(psych)
library(DT)
library(reshape2)
require(FactoMineR)
df_sb <- read.csv('./starbucks/StarbucksStores.txt', stringsAsFactors = FALSE)
df_sb_unique <- df_sb[!duplicated(df_sb), ]
#PCA procedure to determine which amenities should be enabled for filtering on the map
df_sb_pca <- df_sb_unique[,c(11:ncol(df_sb_unique))] #select the amenities columns
df_sb_pca <- df_sb_pca[,-ncol(df_sb_pca)] #drop the last column 'AmeOther'
#head(df_sb_unique[,1:7], 1)
#df_sb_pca %>% count() #number of stores
#amenMean <- map_dbl(df_sb_pca,sd)
#amenMean <- sort(amenMean,decreasing = TRUE)
#amenMean <- as.data.frame(amenMean, row.names = rownames(amenMean))
#amenMean
#amenSd <- map_dbl(df_sb_pca,sd)
#amenSd <- sort(amenSd,decreasing = TRUE)
#amenSd <- as.data.frame(amenSd, row.names = rownames(amenSd))
#amenSd
amenSums <- map_dbl(df_sb_pca, sum)
amenSums <- sort(amenSums,decreasing = TRUE)
amenSums <- as.data.frame(amenSums, row.names = rownames(amenSums))
#amenSums
#amenStatsCombined <- cbind(amenSums,amenMean)
#amenStatsCombined <- cbind(amenStatsCombined, amenSd)
#menStatsCombined
#Data table
datatable(df_sb_unique, options = list(
searching = TRUE,
pageLength = 5,
lengthMenu = c(5, 10, 15, 20)
))
glimpse(amenSums)
df_sb_subset <- df_sb_unique[,c(2:32)]
df_sb_subset <- df_sb_subset[,-c(2:9)]
mdata <- melt(df_sb_subset, id=c("Name"))
amenSums$amenities <- rownames(amenSums)
glimpse(amenSums)
p1 <- ggplot(amenSums,aes(amenities,amenSums)) + #, label = amenSums$amenSums
geom_bar(stat = "identity", aes(fill=amenities)) +
ylab("Count") +
xlab("Amenities by Code") +
ggtitle("Starbucks Coffee Store Amenities") +
theme(legend.position="none") +
coord_flip()
ggplotly(p1)
reorder(amenities,amenSums)
reorder(amenSums$amenities,amenSums$amenSums)
amenSums[reorder(amenSums$amenities,amenSums$amenSums),]
amenSum.reordered <- amenSums[reorder(amenSums$amenities,amenSums$amenSums),]
p1 <- ggplot(amenSum.reordered,aes(amenities,amenSums)) + #, label = amenSums$amenSums
geom_bar(stat = "identity", aes(fill=amenities)) +
ylab("Count") +
xlab("Amenities by Code") +
ggtitle("Starbucks Coffee Store Amenities") +
theme(legend.position="none") +
coord_flip()
ggplotly(p1)
amenSum.reordered
ggplot(amenSum.reordered, aes(amenities,amenSums)) + geom_bar(stat='identity')
df_sb_subset <- df_sb_unique[,c(2:32)]
df_sb_subset <- df_sb_subset[,-c(2:9)]
mdata <- melt(df_sb_subset, id=c("Name"))
amenSums$amenities <- rownames(amenSums)
p1 <- ggplot(amenSums,aes(reorder(amenities,amenSums),amenSums)) + #, label = amenSums$amenSums
#amenSum.reordered <- amenSums[reorder(amenSums$amenities,amenSums$amenSums),]
#p1 <- ggplot(amenSum.reordered,aes(amenities,amenSums)) + #, label = amenSums$amenSums
geom_bar(stat = "identity", aes(fill=amenities)) +
ylab("Count") +
xlab("Amenities by Code") +
ggtitle("Starbucks Coffee Store Amenities") +
theme(legend.position="none") +
coord_flip()
ggplotly(p1)
install.packages("HSAUR")
library(HSAUR)
data("heptathlon")
library(tidyverse)
glimpse(heptathlon)
library(fBasics)
glimpse(heptathlon)
summary(heptathlon)
map_dbl(heptathlon,sd)
plot(heptathlon)
hep <- heptathlon
glimpse(hep)
map(hep$hurdles,function(x) max(x) - x)
max(hep$hurdles)
max(hep$hurdles) - hep$hurdles
hep$hurdles <- max(hep$hurdles) - hep$hurdles
hep$run200m <- max(hep$run200m) - hep$run200m
hep$run800m <- max(hep$run800m) - hep$run800m
plot(hep)
plot(hep)
names(hep)
fa.parallel(hep[,-8])
nrow(hep)
fa.parallel(hep[,-8],
n.obs = nrow(hep),
fa = 'pc',
n.iter = 100)
abline(h=1)
fa.parallel(hep[,-8],
n.obs = nrow(hep),
fa = 'pc',
n.iter = 100)
abline(h=1)
pc_hep = principal(hep[,-8], #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_hep
factor.plot(pc_hep,
labels = colnames(hep)[1:7])
plot(pc_hep$values)
plot(pc_hep$scores)
plot(hep[,-8])
which(hep$javelin,hep$javelin == max(hep$javelin))
which(hep,hep$javelin == max(hep$javelin))
hep[hep$javelin == max(hep$javelin)),]
hep[hep$javelin == max(hep$javelin),]
influencePlot(pc_hep$scores)
summary(hep$javelin)
scatterplotMatrix(~AG+YR+CD|LC,data=case2002[, c(1,5:7)])
pchisq(glm.case$deviance, glm.case$df.residual, lower.tail = FALSE)
glm.case$coefficients
exp(glm.case$coefficients)
confint.default(glm.case)
exp(confint.default(glm.case))
glm.minus.bird$deviance - glm.minus.bird$fitted.values
pchisq(glm.minus.bird$deviance - glm.minus.bird$residuals, lower.tail = FALSE)
pchisq(glm.minus.bird$deviance - glm.minus.bird$residuals, lower.tail = FALSE)
pchisq(glm.minus.bird$deviance - glm.minus.bird$residuals, lower.tail = FALSE)
summary(glm.minus.bird)
pchisq(glm.minus.bird$deviance - glm.minus.bird$residuals, lower.tail = FALSE)
glm.minus.bird$df.residual
glm.minus.bird$deviance
pchisq(glm.minus.bird$deviance - glm.minus.bird$df.residual, lower.tail = FALSE)
pchisq(glm.minus.bird$deviance - glm.minus.bird$df.residual, lower.tail = FALSE)
pchisq(glm.minus.bird$deviance, glm.minus.bird$df.residual, lower.tail = FALSE)
1 - glm.case$deviance / glm.case$null.deviance
1 - glm.bk.yr$deviance / glm.bk.yr$null.deviance
1 - glm.minus.bird$deviance / glm.minus.bird
1 - glm.minus.bird$deviance / glm.minus.bird$null.deviance
table(truth = case2002$LC, prediction = LC.predicted)
(85 + 22) / (85 +  13 + 27 + 22)
prostate <- read.csv('~/Documents/DataScience/bootcamp7/week7/07TheCurseofDimensionalityHomework/[07] Prostate.txt')
glimpse(prostate)
prostate <- read.table('~/Documents/DataScience/bootcamp7/week7/07TheCurseofDimensionalityHomework/[07] Prostate.txt')
glimpse(prostate)
plot(prostate)
training = prostate[sample(prostate,prob = .80),]
training = prostate[sample(prostate,size = nrow(prostate) *.80),]
training = prostate[sample(prostate,size = nrow(prostate) *.80, replace = FALSE),]
x
train = sample(1:nrow(prostate), 8*nrow(prostate)/10)
dim(train)
length(train)
length(prostate)
dim(prostate)
training = prostate[train,]
dim(training)
test = prostate[-train,]
length(test)
dim(train)
class(train)
length(training)/nrow(prostate)
-train
train
glimpse(prostate)
x.training = prostate[train, -9]
glimpse(x.training)
y.training = prostate[train, 9]
glimpse(y.training)
x.test = prostate[-train, -9]
y.test = prostate[-train, 9]
grid = 10^seq(5, -2, length = 100)
ridge.models.train = glmnet(x.training, y.training, alpha = 0, lambda = grid)
library(glmnet)
ridge.models.train = glmnet(x.training, y.training, alpha = 0, lambda = grid)
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
library(ISLR)
Hitters = na.omit(Hitters)
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
grid = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
coef(ridge.models) #Inspecting the various coefficient estimates.
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
predict(ridge.models, s = 50, type = "coefficients")
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
class(x)
typeof(x)
x
glimpse(Hitters)
model.matrix(Salary ~ ., Hitters)
head(model.matrix(Salary ~ ., Hitters)
)
names(prostate)
x = model.matrix(lpsa ~ ., prostate)[, -1] #Dropping the intercept column.
x
head(x)
y = prostate$lpsa
y
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
test
y.test
length(train)/nrow(x)
length(y.test)/nrow(x)
train = sample(1:nrow(x), 8*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
ridge.models$lambda
ridge.models$lambda[80]
plot(cv.ridge.out, main = "Ridge Regression\n")
plot(ridge.models.train, main = "Ridge Regression\n")
x = model.matrix(lpsa ~ ., prostate)[, -1] #Dropping the intercept column.
y = prostate$lpsa
grid = 10^seq(5, -2, length = 100)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
ridge.models$lambda[80]
plot(ridge.models.train, main = "Ridge Regression\n")
bestlambda.ridge = ridge.models.train$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid, nfolds = 10)
ridge.models.train = cv.glmnet(x[train, ], y[train], alpha = 0, lambda = grid, nfolds = 10)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
ridge.models$lambda[80]
plot(ridge.models.train, main = "Ridge Regression\n")
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
ridge.models$lambda[80]
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
coef(ridge.models) #Inspecting the various coefficient estimates.
y
plot(ridge.models.train, xvar = "lambda", label = TRUE, main = "Ridge Regression")
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
ridge.models.train$lambda[80]
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
ridge.out = glmnet(x, y, alpha = 0)
predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
ridge.out.pred <- predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
ridge.out.pred
ridge.bestlambda = predict(ridge.out, s = bestlambda.ridge, newx = x)
mean((ridge.bestlambda - y)^2)
lasso.models = glmnet(x, y, alpha = 1, lambda = grid)
dim(coef(lasso.models)) #20 different coefficients, estimated 100 times --
coef(lasso.models) #Inspecting the various coefficient estimates.
lasso.models$lambda[80] #Lambda = 0.2595.
coef(lasso.models)[, 80] #Most estimates not close to 0.
sum(abs(coef(lasso.models)[-1, 80])) #L1 norm is 228.1008.
lasso.models$lambda[15] #Lambda = 10,235.31.
sum(abs(coef(lasso.models)[-1, 15])) #L1 norm is essentially 0.
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
cv.lasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
cv.lasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cv.lasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
lasso.bestlambdatrain = predict(lasso.models, s = bestlambda.lasso, newx = x[test, ])
mean((lasso.bestlambdatrain - y.test)^2)
lasso.out = glmnet(x, y, alpha = 1)
predict(lasso.out, type = "coefficients", s = bestlambda.lasso)
#Let's also inspect the MSE of our final lasso model on all our data.
lasso.bestlambda = predict(lasso.out, s = bestlambda.lasso, newx = x)
mean((lasso.bestlambda - y)^2)
glm.case$deviance
glm.case$null.deviance
1 - glm.case$deviance / glm.case$null.deviance
summary(glm.case)
library(ISLR)
data(OJ)
glimpse(OJ)
map_dbl(OJ[,-c(1,14)])
map_dbl(OJ[,-c(1,14)],sd)
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
tree.carseats
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
#Inspecting the elements of the cv.tree() object.
names(cv.carseats)
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#Pruning the overall tree to have 4 terminal nodes; choose the best tree with
#4 terminal nodes based on cost complexity pruning.
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
#Assessing the accuracy of the pruned tree with 4 terminal nodes by constructing
#a confusion matrix.
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(53 + 33) / (53 + 33 + 21 + 13)
library(MASS)
help(Boston)
#Creating a training set on 70% of the data.
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
library(randomForest)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
help(Carseats)
attach(Carseats)
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
head(Carseats)
str(Carseats)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
